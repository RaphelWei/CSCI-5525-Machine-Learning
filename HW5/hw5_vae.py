import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from torch import nn
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import MNIST
from torchvision.utils import save_image


# hyperparameters initialization
epoch_num = 10
batch_size = 64
lr = 0.0002


# read data
transform = transforms.Compose([transforms.ToTensor(),])
trainset = MNIST(root = "./data/", transform = transform, train = True, download = True)
testset = MNIST(root="./data/", transform = transform, train = False, download = True)
dataloader_train = DataLoader(dataset=trainset, batch_size = batch_size, shuffle=True)
dataloader_test = DataLoader(dataset=testset, batch_size = batch_size, shuffle=True)

class VAE(nn.Module):
    """
    This is a class for variational auto-encoder
    Encoder - layer: fc-ReLu
            - mu (fc) and sigma (fc)
    ##############################################
    Decodder - fc-ReLu-fc-Sigmoid
    """
    def __init__(self):
        super(VAE, self).__init__()
        self.layer = nn.Sequential(nn.Linear(784, 400),
                                     nn.ReLU())
        
        self.mu = nn.Linear(400, 20)
        self.sigma = nn.Linear(400, 20)
        
        self.decoder = nn.Sequential(nn.Linear(20, 400),
                                     nn.ReLU(),
                                     nn.Linear(400, 784),
                                     nn.Sigmoid()
                                     )


    def combine(self, mu, sigma):
        """
        This is a function to combine mu and sigma
        the latent variable z = mu + epsilon * sigma, 
        where epsilon is a random normal noise
        """
        eps = torch.randn(mu.size(0), mu.size(1))
        z = eps * sigma + mu
        return z

    def forward(self, x):
        """
        Forward Propogation function
        """
        x = self.layer(x) 
        mu = self.mu(x)
        sigma = self.sigma(x)
        z = self.combine(mu, sigma)  # get latent variable z        
        new_x = self.decoder(z) # get reconstructed image new_x
        return new_x, mu, sigma

def plot_img(img):
    """
    This is a function to plot reconstructed images
    """
    plt.figure(figsize=[6, 6])
    for i in range(4*4):
        plt.subplot(4, 4, i+1)
        plt.imshow(img[i].reshape(28,28), cmap='gray') # for each image, reshape it to 28x28 and plot it
        frame = plt.gca() # eliminate the axises
        frame.axes.get_xaxis().set_visible(False)
        frame.axes.get_yaxis().set_visible(False)
    # adjust the figure
    plt.subplots_adjust(wspace =0.05, hspace=0.05)
    plt.savefig('p6_recons_image.eps')
    plt.show()


# parameters initialization
vae = VAE() # the model we would like to train
vae_optimizer = torch.optim.Adam(vae.parameters(), lr=lr) # using ADAM as optimizer

def criterion(x, mu, sigma, new_x):
    """
    This is a our loss function
    @params:
    x - original imput image
    mu - mu generated by VAE
    sigma - sigma generated by VAE
    new_x - reconstructed image from VAE
    """
    # th loss consists of 2 componentsï¼š
    # BCE loss and KL divergence
    BCE_loss = F.binary_cross_entropy(new_x, x, reduction='sum')
    KL_Divergence = -0.5 * torch.sum(1 + sigma.pow(2).log() - mu.pow(2) - sigma.pow(2))
    loss = BCE_loss + KL_Divergence

    return loss/batch_size


def train_VAE():
    """
    This is a function used to train autoencoder
    return - losses: a list consisting average loss of each epoch
    """
    losses = [] # initialize losses as an empty list
    for epoch in range(epoch_num):
        sum_loss = 0 # total loss for this epoch
        for _, (img, _) in enumerate(dataloader_train):
            # img is the training image, we need to reshape it first
            img = img.view(img.size(0), -1)
            # Forward propagation
            # get reconstructed image and sigma, mu 
            re_img, mu, sigma = vae(img)
            # get loss of this iteration
            loss = criterion(img, mu, sigma, re_img)
            sum_loss += loss.item() * batch_size
        
            # Backpropagation, update weights
            vae_optimizer.zero_grad()
            loss.backward()
            vae_optimizer.step()
        # After an epoch finished, print some info
        print('Epoch [{}/{}], Loss:{:.6f}'.format(epoch+1, epoch_num, sum_loss / 60000))
        losses.append(sum_loss / 60000)
    return losses

# Get the loss list
losses = train_VAE()

# Plot average loss for each epoch
plt.plot(np.arange(1, epoch_num+1), losses)
plt.savefig("p6_loss.eps")
plt.show()

torch.save(vae, 'hw5_vae.pth')


"""
The following code are used to plot noise image and reconstructed image
"""
# reload our model
model = torch.load('hw5_vae.pth')
# randomly pick 16 images from test set
# here we just use index [0] because the test data has been shuffled
img_test = next(iter(dataloader_test))[0].data[:16].view(16, -1) 
re_img_test, _, _ = model(img_test) # get the reconstructed test images using our model
plot_img(re_img_test.data)

